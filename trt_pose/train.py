import argparse
import subprocess
import torch
import torchvision
import os
import tqdm
import apex.amp as amp
import time
from .data import PoseDataset
from .models import MODELS


if __name__ == '__main__':
    
    parser = argparse.ArgumentParser()
    parser.add_argument('--model', type=str, default='resnet18_u256', help='The model name')
    parser.add_argument('--init_weights', type=str, default='', help='Path to pre-trained weights for model')
    parser.add_argument('--epochs', type=int, default=50, help='Number of training epochs')
    parser.add_argument('--batch_size', type=int, default=128, help='Training batch size')
    parser.add_argument('--dataset', type=str, default='dataset', help='Path to dataset generated by generate_pose_dataset')
    parser.add_argument('--output_dir', type=str, default='checkpoints', help='Path to store saved model weights')
    parser.add_argument('--save_interval', type=int, default=5, help='Save interval in epochs')
    parser.add_argument('--num_loader_workers', type=int, default=8)
    args = parser.parse_args()
    
    if not os.path.exists(args.output_dir):
        subprocess.call(['mkdir', '-p', args.output_dir])
        
    dataset = PoseDataset(
        args.dataset,
        transforms=torchvision.transforms.Compose([
            torchvision.transforms.ToTensor(),
            torchvision.transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
        ])
    )
    
    # infer shape from dataset
    _, cmap, paf = dataset[0]
    cmap_channels = int(cmap.shape[0])
    paf_channels = int(paf.shape[0])
    
    device = torch.device('cuda')
    model = MODELS[args.model](cmap_channels, paf_channels)
    if len(args.init_weights) > 0:
        model.load_state_dict(torch.load(args.init_weights))
    model = model.to(device)
    
    loader = torch.utils.data.DataLoader(
        dataset,
        batch_size=args.batch_size,
        shuffle=True,
        num_workers=args.num_loader_workers
    )
    
    optimizer = torch.optim.Adam(model.parameters())
    
    model, optimizer = amp.initialize(model, optimizer, opt_level="O1")

    model = model.train()
    for i in range(args.epochs):
        
        t0 = time.time()
        
        epoch_loss = 0.0
        j = 0
        
        for image, cmap, paf in tqdm.tqdm(iter(loader)):
            optimizer.zero_grad()

            image = image.to(device)
            cmap = cmap.to(device)
            paf = paf.to(device)

            cmap_out, paf_out = model(image)

            cmap_mask = cmap.max(1, keepdim=True)[0]
            paf_mask = (paf[:, 0::2]**2 + paf[:, 1::2]**2).sqrt().max(1, keepdim=True)[0]

            cmap_loss = torch.mean((1e-2 + cmap_mask) * (cmap_out - cmap)**2)
            paf_loss = torch.mean((1e-2 + paf_mask) * (paf_out - paf)**2)

            loss = cmap_loss + paf_loss
            epoch_loss += float(loss)
            
            with amp.scale_loss(loss, optimizer) as scaled_loss:
                scaled_loss.backward()
            optimizer.step()

            j += 1
            
        t1 = time.time()
        print('%d (%fsec), %f' % (i, (t1 - t0), epoch_loss / len(loader)))
        
        if i % args.save_interval == 0:
            checkpoint_path = os.path.join(args.output_dir, 'epoch_%d.pth')
            torch.save(model.state_dict(), checkpoint_path)